{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отработка ошибок, связанных с запуском кода\n",
    "\n",
    "- Если после установки необходимых библиотек возникает ошибка, то скорее всего не установлен вебдрайвер Chrome для вашей ОС. Как установить вебдрайвер можно ознакомиться по этой ссылке. [selenium-python](https://selenium-python.com/install-chromedriver-chrome?ysclid=ly2ufnhjip111135754)\n",
    "- Если после установки вебдрайвера код не отрабатывает либо возникает ошибка, то требуется проверить не открыт ли в памяти браузер Chrome. Если да, то требуется его выгрузить (закрыть). Это связано с тем, что данный код отрабатывает в фоновом режиме, но при этом использует вебдрайвер Chrome который может конфликтовать с самим запущенным браузером Chrome.\n",
    "- Если ошибка по-прежнему возникает, то требуется проверить или переустановить используемые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pip\n",
    "#!pip3 install fake-useragent\n",
    "#!pip3 install selenium_stealth\n",
    "#!pip3 install selenium\n",
    "#!pip3 install pandas\n",
    "#!pip3 install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from lxml import html\n",
    "from datetime import datetime\n",
    "from selenium_stealth import stealth\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser_olx:\n",
    "\n",
    "    # инициализатор \n",
    "    def __init__(self, sleep_time_min = 30, sleep_time_max = 120, upload_wait_time = 5, parsing_stopper_min = 300, parsing_stopper_max = 600, parsing_stopper_count = 10):\n",
    "        # инициализируем врмя задержки между действиями\n",
    "        self.sleep_time_min = sleep_time_min\n",
    "        self.sleep_time_max = sleep_time_max\n",
    "        # инициализируем время ожидания загрузки данных\n",
    "        self.upload_wait_time = upload_wait_time\n",
    "        # инициализируем время ожидания между загрузкой блока данных\n",
    "        self.parsing_stopper_min = parsing_stopper_min\n",
    "        self.parsing_stopper_max = parsing_stopper_max\n",
    "        # инициализируем объем блока данных между стопами\n",
    "        self.parsing_stopper_count = parsing_stopper_count\n",
    "    \n",
    "    # функция инициализации вебдрайвера с заданными параметарми\n",
    "    def webdriver(self):\n",
    "        # инициализируем юзер агент для сокрытия реальных данных браузера для избежания блокировки\n",
    "        ua = UserAgent(browsers='chrome')\n",
    "        # берем рандомный параметр пользовательского агента\n",
    "        user_agent = ua.random\n",
    "        # инициализируем оъект опций драйвера\n",
    "        options = Options()\n",
    "        # режим работы в фоновом режиме\n",
    "        options.add_argument('--headless=new')\n",
    "        # присваивание выбранного пользовательского агента \n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        # задаем максимальный размер окна чтобы сайт не переходил на работу в режиме мобильной версии в которой элементы не подгружаются тк расположены иначе\n",
    "        options.add_argument('start-maximized')\n",
    "        # присваеваем окну конкретный размер\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        # задаем параметр отработки графики только на процессоре\n",
    "        options.add_argument('--disable-gpu')\n",
    "        # задаем параметр отключение хранения временных файлов\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        # отключение уведомлений в браузере\n",
    "        options.add_argument('--disable-notifications')\n",
    "        # отключаем блокировку всплывающих окон\n",
    "        options.add_argument('--disable-popup-blocking')\n",
    "        # отключаем режим песочницы\n",
    "        options.add_argument('--no-sandbox')\n",
    "        # активация использования браузеру джава скрипта\n",
    "        options.add_argument('--enable-javascript')\n",
    "        # отклчение детекции джава скриптом браузера как браузера под автоматическоим управлением\n",
    "        options.add_argument( '--disable-blink-features=AutomationControlled')\n",
    "        # добавляется экспериментальная опция, исключающая переключатель \"enable-automation\", чтобы предотвратить автоматическое обнаружение, что браузер управляется вебдрайвером\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "        # добавляется экспериментальная опция, отключающая расширение автоматизации в браузере, что также помогает скрыть использование вебдрайвера\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        # инициализируем вебдрайвер с заданными ранее опциями\n",
    "        driver = Chrome(options=options)                               \n",
    "        # настройка селениум стелс режим для анаоноимности браузера\n",
    "        stealth(driver,\n",
    "                user_agent=user_agent,\n",
    "                languages=['ru-RU', 'ru'],\n",
    "                vendor='Google Inc.',\n",
    "                platform=user_agent.split(' ')[1].strip('()'),\n",
    "                webgl_vendor='Intel Inc.',\n",
    "                renderer='Intel Iris OpenGL Engine',\n",
    "                fix_hairline=True,\n",
    "                run_on_insecure_origins=True\n",
    "                )\n",
    "        # замена прототипа браузера для исключения детекции браузера под автоматическим управлением\n",
    "        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "            'source':\n",
    "            '''\n",
    "            const newProto = navigator.__proto__;\n",
    "            delete newProto.webdriver;\n",
    "            navigator.__proto__ = newProto;\n",
    "            '''\n",
    "            }\n",
    "        )\n",
    "        # удаляем все куки если они были ранее\n",
    "        driver.delete_all_cookies()\n",
    "        # Установка размеров окна\n",
    "        driver.set_window_size(1920, 1080)\n",
    "        # центровка расположения контента\n",
    "        driver.set_window_position(0, 0)\n",
    "        return driver\n",
    "        \n",
    "    # функция проверки корректности URL\n",
    "    def check_url(self, url):\n",
    "        try:\n",
    "            # отправляет запрос для проверки статуса url, ответ 200 страница существует\n",
    "            response = requests.head(url)\n",
    "            if response.status_code == 200:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except requests.RequestException:\n",
    "            return False\n",
    "\n",
    "    # функция парсинга ссылок результатов со страницы поиска\n",
    "    def page_search_parser(self, driver, url_request_page):\n",
    "        # передаем адрес страницы поиска в драйвер\n",
    "        driver.get(url_request_page)\n",
    "        # ожидание загрузки данных\n",
    "        wait = WebDriverWait(driver, self.upload_wait_time)\n",
    "        # скролим страницу до конца чтобы загрузились все элементы\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        # ожидания загрузки элемента для парсинга\n",
    "        wait.until(EC.visibility_of_element_located((By.XPATH, \"//div[@data-testid='l-card']\")))\n",
    "        # забираем загруженную страницу из драйвера для поиска нужных элементов\n",
    "        page = html.fromstring(driver.page_source)\n",
    "        # извлекаем все элементы с указанным путем\n",
    "        elements = page.xpath(\"//div[@data-testid='l-card']/div/div/div/a/@href\")\n",
    "        # возвращаем массив текстового содержимого элементов с восстановленной сылкой\n",
    "        results = [('https://www.olx.uz' + str(element)) for element in elements]\n",
    "        return results\n",
    "\n",
    "    # функция определяющая номер страницы\n",
    "    def url_number_page(self, url_request):\n",
    "        # захват групп цифр после page=\n",
    "        match = re.search(r'page=(\\d+)', url_request)\n",
    "        if match:\n",
    "            # берет только первую группу цифр из регулярки тк остальные не относятся к номеру страницы\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            # если элемент page= не найден то сылка является первой\n",
    "            return 1\n",
    "    \n",
    "    # функция генерации новых страниц запроса\n",
    "    def search_parser(self, url_request):\n",
    "        # проверяем страницу на факт существования\n",
    "        if not self.check_url(url_request):\n",
    "            print(f'Ccылка не существует: {url_request}')\n",
    "            # возвращаем пустой массив\n",
    "            return []\n",
    "        # создаем экземпляр вебдрайвера\n",
    "        driver = self.webdriver()\n",
    "        # обозначаем номер первой страницы\n",
    "        page = 1\n",
    "        # парсинг первой страницы запроса\n",
    "        url_request_array = self.page_search_parser(driver, url_request)\n",
    "        # проверяем страницу на факт того что выбрана первая тсраница поиска если нет то парсим введенную страницу и останавливаем ссбор сылок\n",
    "        if 'page=' in url_request:\n",
    "            print(f'Задана не первая страница запроса тк URL сожержит элемет page=. Парсер собрал данные только с этой страницы а не со всего запроса: {url_request}')\n",
    "            # удаляем дубликаты собранных сылок\n",
    "            url_request_array_result = list(set(url_request_array))\n",
    "            print(f'Парсинг страниц сылок завершен. Найдено ссылок: {len(url_request_array_result)}')\n",
    "            # закрываем вебдрайвер\n",
    "            driver.quit()\n",
    "            # возвращаем собранные ссылки\n",
    "            return url_request_array_result\n",
    "        print(f'Ссылки собраны со страницы: {page}')\n",
    "        # цикл парсинга без условия выхода\n",
    "        while True:\n",
    "            # увеличиваем счетчик с каждой итерацией\n",
    "            page += 1\n",
    "            # проверяем кейс которому соответствует сылка тк от этого зависит генерация новых сылок\n",
    "            if 'search%' not in url_request:\n",
    "                if '/?currency=' not in url_request:\n",
    "                    # генерация новой сылки согласно логики выбранного кейса search нет в ссыле currency нет в ссылке\n",
    "                    new_url_request = url_request + '?page=' + str(page) if url_request[-1] == '/' else url_request + '/?page=' + str(page)\n",
    "                else:\n",
    "                    # генерация новой сылки согласно логики выбранного кейса search нет в ссыле currency есть в ссылке\n",
    "                    new_url_request = url_request[:-1] + '&page=' + str(page) if url_request[-1] == '/' else url_request + '&page=' + str(page)\n",
    "            else:\n",
    "                url_request_parts = url_request.split('search%')\n",
    "                # генерация новой сылки согласно логики выбранного кейса search есть в ссыле currency нет или есть в ссылке\n",
    "                new_url_request = url_request_parts[0] + 'page=' + str(page) + '&search%' + url_request_parts[1]\n",
    "            # парсинг данных сгенерированной сылки           \n",
    "            new_page_array = self.page_search_parser(driver, new_url_request)\n",
    "            # условия выхода из цикла, открыта страница номер меньше чем передавалась в драйвер или 50 итераций с момента старта как защита от бесконечного цикла\n",
    "            if self.url_number_page(driver.current_url) < page or page == 51:\n",
    "                break\n",
    "            # добавление в массив данных с новой страницы\n",
    "            url_request_array.extend(new_page_array)\n",
    "            print(f'Ссылки собраны со страницы: {page}')\n",
    "        # удаляем дубликаты собранных сылок\n",
    "        url_request_array_result = list(set(url_request_array))\n",
    "        print(f'Парсинг страниц сылок завершен. Найдено уникальных ссылок: {len(url_request_array_result)}')\n",
    "        # закрываем вебдрайвер\n",
    "        driver.quit()\n",
    "        # возвращаем собранные ссылки\n",
    "        return url_request_array_result\n",
    "    \n",
    "    # функция сбора данных со страницы объявления тк классы на странице используют динамические css наименования и могут измениться использую пути селекторов для автотестов и XPATH пути от найденных элементотв.\n",
    "    def page_parser(self, parsing_url, page):\n",
    "        page_data = {}\n",
    "        page_data['page_url'] = parsing_url\n",
    "        try:\n",
    "            page_data['user_type'] = page.xpath('//*[@data-testid=\"main\"]/div[2]/ul/li[1]/p/span/text()')[0]\n",
    "        except:\n",
    "            page_data['user_type'] = None\n",
    "        try:\n",
    "            page_data['post_teg'] = ', '.join(page.xpath('//*[@data-testid=\"main\"]/div[2]/ul/li/p/text()'))\n",
    "        except:\n",
    "            page_data['post_teg'] = None\n",
    "        try:    \n",
    "            page_data['description'] = re.sub(r'\\s+', ' ', (''.join(page.xpath('//*[@data-testid=\"ad_description\"]/div/text()'))).replace('\\n', '').strip()).replace('\\t ', '\\t').replace(' \\t', '\\t')\n",
    "        except:\n",
    "            page_data['description'] = None\n",
    "        try:\n",
    "            page_data['id'] = page.xpath('//*[@data-cy=\"ad-footer-bar-section\"]/span[1]/text()[2]')[0]\n",
    "        except:\n",
    "            page_data['id'] = None\n",
    "        try:\n",
    "            page_data['views'] = page.xpath('//*[@data-testid=\"page-view-counter\"]/text()')[0].split(': ')[1]\n",
    "        except:\n",
    "            page_data['views'] = None\n",
    "        try:\n",
    "            page_data['user_name'] = page.xpath('//*[@data-testid=\"user-profile-link\"]/div/div[2]/h4/text()')[0]\n",
    "        except:\n",
    "            page_data['user_name'] = None\n",
    "        try:\n",
    "            page_data['user_link'] = 'https://www.olx.uz' + page.xpath('//*[@data-testid=\"user-profile-link\"]/@href')[0]\n",
    "        except:\n",
    "            page_data['user_link'] = None\n",
    "        try:\n",
    "            page_data['user_registered'] = page.xpath('//*[@data-testid=\"user-profile-link\"]/div/div[2]/p/span/text()')[0].replace(' г.', '')\n",
    "        except:\n",
    "            page_data['user_registered'] = None\n",
    "        try:\n",
    "            page_data['user_online_date'] = page.xpath('//*[@data-testid=\"lastSeenBox\"]/span/text()')[0].replace('Онлайн ', '').replace(' г.', '')\n",
    "        except:\n",
    "            page_data['user_online_date'] = None\n",
    "        try:\n",
    "            page_data['posted'] = page.xpath('//*[@data-cy=\"ad-posted-at\"]/text()')[0]\n",
    "        except:\n",
    "            page_data['posted'] = None\n",
    "        try:\n",
    "            page_data['post_name'] = page.xpath('//*[@data-testid=\"ad_title\"]/h4/text()')[0]\n",
    "        except:\n",
    "            page_data['post_name'] = None\n",
    "        try:\n",
    "            page_data['cost'] = page.xpath('//*[@data-testid=\"ad-price-container\"]/h3/text()')[0]\n",
    "        except:\n",
    "            page_data['cost'] = None\n",
    "        try:\n",
    "            page_data['haggle'] = page.xpath('//*[@data-testid=\"ad-price-container\"]/p/text()')[0]\n",
    "        except:\n",
    "            page_data['haggle'] = None\n",
    "        try:\n",
    "            page_data['phone'] = page.xpath('//*[@data-testid=\"contact-phone\"]/text()')[0].replace(' ', '').replace('+', '')\n",
    "        except:\n",
    "            page_data['phone'] = None\n",
    "        try:\n",
    "            page_data['reviews'] = page.xpath('//*[@data-testid=\"sentiment-description\"]/text()')[0]\n",
    "        except:\n",
    "            page_data['reviews'] = None\n",
    "        try:\n",
    "            page_data['rating'] = page.xpath('//*[@data-testid=\"sentiment-title\"]/text()')[0]\n",
    "        except:\n",
    "            page_data['rating'] = None\n",
    "        try:\n",
    "            page_data['category'] = ' | '.join(page.xpath('//*[@data-testid=\"breadcrumb-item\"]/a/text()'))\n",
    "        except:\n",
    "            page_data['category'] = None\n",
    "        try:\n",
    "            user_address_part = page.xpath('//*[@id=\"mainContent\"]/div/div[2]/div[3]/div[2]/div[3]/div/section/div[1]/div/p[1]/span/text()')\n",
    "            if user_address_part:\n",
    "                page_data['user_address'] = ''.join(page.xpath('//*[@data-testid=\"aside\"]/div[3]/div/section/div[1]/div/p/text()')) + ', ' + user_address_part[0]\n",
    "            else:\n",
    "                page_data['user_address'] = ''.join(page.xpath('//*[@data-testid=\"aside\"]/div[3]/div/section/div[1]/div/p/text()'))\n",
    "        except:\n",
    "            page_data['user_address'] = None\n",
    "        # добавляем признак собранных данных для удобства отработки ошибок\n",
    "        try:\n",
    "            page_data['data_parsed'] = 'phone_not_parsed' if page_data['phone'] is None and page.xpath('//*[@data-testid=\"ad-contact-phone\"]') else None\n",
    "        except:\n",
    "            page_data['data_parsed'] = None\n",
    "        return page_data\n",
    "    \n",
    "    # функция для прокликивания элементов страницы и возврата блока парсинга страницы\n",
    "    def clicker(self, parsing_url):\n",
    "        # проверяем URL на факт существования\n",
    "        if self.check_url(parsing_url):\n",
    "            # создаем экземпляр вебдрайвера\n",
    "            driver = self.webdriver()\n",
    "            # передаем url страницы в драйвер\n",
    "            driver.get(parsing_url)\n",
    "            # не явное ожидание загрузки страницы \n",
    "            driver.implicitly_wait(self.upload_wait_time)\n",
    "            # расчет рандомного времени ожидания\n",
    "            random_wait_time = random.uniform(self.sleep_time_min, self.sleep_time_max)\n",
    "            print(f'Время ожидания: {random_wait_time}')\n",
    "            # применяем ожидание \n",
    "            time.sleep(random_wait_time / 2)\n",
    "            # скролим страницу до конца чтобы загрузились все элементы\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # применяем ожидание \n",
    "            time.sleep(random_wait_time / 2)\n",
    "            # скролим страницу до конца вверх чтобы загрузились все элементы с которыми будем взаимодействовать\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            # ожидание загрузки данных\n",
    "            wait = WebDriverWait(driver, self.upload_wait_time)\n",
    "            # ждем загрузки конкретных элементов страницы после прокликивания\n",
    "            try:\n",
    "                wait.until(EC.visibility_of_element_located((By.XPATH, \"//div[@data-testid='ad_description']\")))\n",
    "                wait.until(EC.visibility_of_element_located((By.XPATH, \"//a[@data-testid='user-profile-link']\")))\n",
    "            except:\n",
    "                print('не найдены элемент описания или профиля')\n",
    "            # пытаемся взаимодействовать с кнопкой\n",
    "            try:\n",
    "                # ищем кнопку\n",
    "                button = driver.find_element(By.XPATH, \"//button[@data-testid='ad-contact-phone']\")\n",
    "                # скролим до кнопки\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n",
    "                # нажимаем на кнопку через JS\n",
    "                driver.execute_script(\"arguments[0].click();\", button)\n",
    "                print('кнопка нажата')\n",
    "                # ждем появление результата нажатия\n",
    "                wait.until(EC.presence_of_element_located((By.XPATH, \"//a[@data-testid='contact-phone']\")))\n",
    "            # повторная попытка взаимодействия например если структура DOM изменилась и элемент был удален и добавлен повторно\n",
    "            except StaleElementReferenceException:\n",
    "                print('кнопка ищется повторно')\n",
    "                # не явное ожидание загрузки страницы \n",
    "                driver.implicitly_wait(self.upload_wait_time)\n",
    "                # ищем кнопку\n",
    "                button = driver.find_element(By.XPATH, \"//button[@data-testid='ad-contact-phone']\")\n",
    "                # скролим до кнопки\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n",
    "                # нажимаем на кнопку через JS\n",
    "                driver.execute_script(\"arguments[0].click();\", button)\n",
    "                print('кнопка нажата')\n",
    "                # ждем появление результата нажатия\n",
    "                wait.until(EC.presence_of_element_located((By.XPATH, \"//a[@data-testid='contact-phone']\")))\n",
    "            # в случае если этемент ожидания не дожидается появления результата то вызывается сообщение об ошибке\n",
    "            except TimeoutException:\n",
    "                print('поймана капча')\n",
    "            # в случае не нахождения кнопки вызывается сообщение об ошибке\n",
    "            except NoSuchElementException:\n",
    "                print('телефон не указан в объявлении')\n",
    "            # передаем содержимое вебдрайвера после взаимодействия со страницей из селениума в формате html\n",
    "            page = html.fromstring(driver.page_source)\n",
    "            # закрываем драйвер\n",
    "            driver.quit()\n",
    "        else:\n",
    "            # передаем пустую строку если страница не существует\n",
    "            page = ''\n",
    "        return page\n",
    "    \n",
    "    # функция парсинга по ссылке поиска объявлений или массиву ранее собранных сылок объявлений \n",
    "    def parser(self, urls):\n",
    "        if isinstance(urls, str):\n",
    "            # запускаем сбор массива сылок\n",
    "            urls = self.search_parser(urls)\n",
    "        elif isinstance(urls, list):\n",
    "            # Массив уже передан в функцию, ничего не меняем\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"Параметр url должен быть ссылкой на страницу данных или списком ранее собранных ссылок\")\n",
    "        # присвоиваем номер страницы\n",
    "        page_number = 0\n",
    "        # присвоиваем номер стопера между блоками собранных данных\n",
    "        parsing_stoper = 0\n",
    "        # создаем хранилище данных\n",
    "        df = []\n",
    "        # цикл парсинга страниц массива\n",
    "        for url_task in urls:\n",
    "            # увеличиваем счетчик\n",
    "            page_number += 1\n",
    "            # увелисиваем стоппер\n",
    "            parsing_stoper += 1\n",
    "            print(f'Данные собираются со страницы: {page_number} {url_task}')\n",
    "            # запуск кликера для сбора номера\n",
    "            clicked_page = self.clicker(url_task)\n",
    "            # запуск парсера собранных страниц\n",
    "            parsed_page = self.page_parser(url_task, clicked_page)\n",
    "            # сохраняем собранные данные в массив\n",
    "            df.append(parsed_page)\n",
    "            # проверяем не поймалили мы капчу или не превышен ли заданный лимит стоппера для вызова ожидания страницы\n",
    "            if parsed_page['data_parsed'] is not None or parsing_stoper == self.parsing_stopper_count:\n",
    "                # генереим случайную величину в ожидании данных в рамках заданного интерала\n",
    "                random_wait_time = random.uniform(self.parsing_stopper_min, self.parsing_stopper_max)\n",
    "                print(f'Пауза для исключения или снятия блокировки {random_wait_time} секунд')\n",
    "                # вызываем ожидание с рассчитанным интералом\n",
    "                time.sleep(random_wait_time)\n",
    "                # обнуляем стопер для корректного вызова повторного ожидания\n",
    "                parsing_stoper = 0              \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с парсером\n",
    "- Для запуска парсинга сайта [olx.uz](https://www.olx.uz/) требуется составить запрос с вызовом парсера Parser_olx().parser(url_request) в данном случае парсер соберет данные сайта со страницы на результаты запроса поиска объявлений url_request. В данном случае парсер запустит сбор ссылок на объявления со страницы адрес, которой введен и автоматически начнет собирать данные с объявлений ссылки на которые были собраны. Результатом работы будет массив данных со ссылок объявлений.\n",
    "- Если url_request был передан первой страницы результата поиска, то парсер сам запустит итератор по остальным страницам и дойдет до конца результата поиска, который выдал сайт. Если url_request содержит не первую страницу, то данные будут собраны только с этой страницы поиска без запуска итератора.\n",
    "- Для удобства парсер можно запускать отдельно сбор ссылок с результата запроса и отдельно сбор данных с самих ссылок объявлений. Такой подход помогает оптимизировать сбор данных, например сначала собрать только ссылки сверить с теми результатами, которые собраны ранее и исключить их из сбора данных оставив только уникальные новые ссылки данные по которым не собирались ранее и запустить сбор данных только с этих ссылок.\n",
    "- Для запуска сбора только ссылок объявлений нужно использовать вызов Parser_olx().search_parser(url_request) где url_request ссылка на первую страницу результата поиска, о остальных страниц итератор соберет данные сам. Если ввести не первую страницу поиска, то будут собраны результаты только с данной страницы. Результатом работы будет массив ссылок объявлений.\n",
    "- Для запуска сбора данных с объявлений нужно использовать вызов Parser_olx().parser(url_array) где url_array массив собранных ссылок объявлений. Результатом работы будет массив данных со ссылок объявлений.\n",
    "- При инициализации класса парсера Parser_olx() можно задавать гиперпараметры которые влияют на скорость работы и задают временные задержки: upload_wait_time - время ожидание загрузки страницы после передачи URL адреса и максимальное время ожидания загрузки элементов на странице после взаимодействия со страницей для подгрузки данных, sleep_time_min и sleep_time_max задают интервал времени между которыми генерируется рандомный интервал ожидания для имитации действий пользователя скролов и аналога ожидания загрузки и чтения информации. parsing_stopper_min и parsing_stopper_max задают интервал времени между которыми генерируется рандомный интервал ожидания для имитации пауз между сбором определенного блока данных или если результат предыдущего сбора данных поймал капчу, parsing_stopper_count определяет количество собираемых данных между паузами. Это помогает быть более похожим на действия человека при сборе информации. Если инициализировать класс без гиперпараметров, то он будет автоматически вызван с параметрами по умолчанию: sleep_time_min = 30, sleep_time_max = 120, upload_wait_time = 5, parsing_stopper_min = 300, parsing_stopper_max = 600, parsing_stopper_count = 10.\n",
    "- Если при сборе информации интересует именно номер телефона пользователя, то не рекомендуется делать слишком маленькие временные интервалы так как на сайте сработает автоматическая защита и сайт будет выдавать капчу в место отображения номера после нажатия на кнопку \"Показать номер\". Капча начнет отображаться ориентировочно после 14 запроса иногда раньше иногда позже. Формат отображаемой капчи ReCAPTCHA. Лучший способ ее обойти это избежать ее вызова. Если система заметит парсинг, то блокировка как правило включится приблизительно на сутки. В этом случае рекомендуется пере подключить Интернет и попробовать повторно или запустить парсер позже. Если капча вызывается слишком часто, то лучше увеличить временные интервалы через гиперпараметры sleep_time_min, sleep_time_max, upload_wait_time, parsing_stopper_min, parsing_stopper_max, parsing_stopper_count.\n",
    "- Если при сборе информации будет вызвана капча, то в колонку data_parsed результирующего фрейма будет занесена запись phone_not_parsed. При необходимости можно будет отфильтровать подобные значения и запустить по ним парсинг повторно.\n",
    "- Если при сборе информации номер телефона пользователя не сильно интересен, например информация интересна для автоматической фильтрации и обработки, а только потом интересен номер телефона и ссылка на объявление то временные задержки sleep_time_min, sleep_time_max, upload_wait_time, parsing_stopper_min, parsing_stopper_max, parsing_stopper_count лучше сделать минимальными для ускорения работы парсера. В таком режиме мы поймаем капчу, но это не будет проблемой так как информация по номеру телефона объявления на этапе сбора информации не будет интересна.\n",
    "- Для сохранения результирующего фрейма в файл CSV требуется раскоментить строки кода #df_pd = pd.DataFrame(df) которая преобразует массив в Data Frame Pandas и #df_pd.to_csv(f\"Selenium_olx_{datetime.now().strftime('%d%m%Y')}.csv\") которая сохранить данные в файл с наименованием \"Selenium_olx_10072024.csv\" в папку с запускаемым кодом. Дата будет подставляться автоматически - текущая дата сохранения результата парсинга.\n",
    "# Важно\n",
    "- Перед использованием парсинга продумайте логику какие данные и для чего Вы планируете собирать и отталкивайтесь от этого устанавливая временные интервалы и задавайте релевантные объемы сбора информации возможно разумнее собирать информацию с иными паузами. Помните время влияет на полученный результат!\n",
    "- У сайта есть ограничение в выдаваемых ссылках. При выдаче результата поиска сайт выдает не все результаты, а чуть больше 1000 наиболее соответствующих запросу. Крайне важно для наиболее хорошего результата сделать наиболее корректный запрос поиска с нужными настройками и фильтрами сайта. Данные настройки автоматом корректируют и добавляются в ссылку запроса. Копируйте ссылку только после формирования корректного запроса, только это обеспечит нужный Вам результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_request = 'https://www.olx.uz/list/q-найди-мне-идеальное-предложение/' #case1\n",
    "#url_request = 'https://www.olx.uz/transport/legkovye-avtomobili/q-красная-машина/?currency=UZS' #case2\n",
    "#url_request = 'https://www.olx.uz/list/q-кофемашина-автоматическая-с-капучинатором/?search%5Bfilter_enum_state%5D%5B0%5D=new' #case3\n",
    "#url_request = 'https://www.olx.uz/transport/legkovye-avtomobili/q-красная-машина/?currency=UZS&search%5Bfilter_enum_car_body%5D%5B0%5D=hatchback' #case4\n",
    "#url_request = 'https://www.olx.uz/transport/legkovye-avtomobili/q-красная-машина/?currency=UZS&page=2&search%5Bfilter_enum_car_body%5D%5B0%5D=hatchback'\n",
    "#url_request = 'https://www.olx.uz/nedvizhimost/kvartiry/'\n",
    "\n",
    "# парсим данные по url страницы поиска результатов\n",
    "#df = Parser_olx().parser(url_request)\n",
    "\n",
    "# для удобства рекомендуется разделять сбор ссылок и сбор даннх со страниц ссылок так комфортнее отрабатывать ошибки или запускать ограниченный, отфильтрованный список сылок например для повторного сбора данных или сбора толко новых данных\n",
    "# парсим url объявлений запроса\n",
    "#url_array = Parser_olx().search_parser(url_request)\n",
    "# парсинг даннх на основе переданного массива url объявлений\n",
    "#df = Parser_olx().parser(url_array)\n",
    "\n",
    "# прелбразовываем df в комфортный для работы с данными формату pandas df\n",
    "#df_pd = pd.DataFrame(df)\n",
    "# сохраняем фрейм в файл текущей датой\n",
    "#df_pd.to_csv(f\"Selenium_olx_{datetime.now().strftime('%d%m%Y')}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_request = 'https://www.olx.uz/nedvizhimost/kvartiry/arenda-dolgosrochnaya/tashkent/?currency=UYE&search%5Bprivate_business%5D=private&search%5Border%5D=filter_float_price:asc&search%5Bfilter_float_number_of_rooms:from%5D=2&search%5Bfilter_float_number_of_rooms:to%5D=5&search%5Bfilter_enum_furnished%5D%5B0%5D=yes&search%5Bfilter_enum_comission%5D%5B0%5D=no'\n",
    "#url_array = Parser_olx().search_parser(url_request)\n",
    "#display(len(url_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем сохраненные сылки из файла\n",
    "read_urls = pd.read_csv('Selenium_olx_array31072024.csv', header=None)\n",
    "data_array = read_urls.iloc[1:, 1].tolist()\n",
    "# запускаем парсинг по нужному блоку сылок\n",
    "df = Parser_olx().parser(data_array[81:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем массив сылок в файл для удобства работы\n",
    "#df_urls = pd.DataFrame(url_array)\n",
    "#df_urls.to_csv(f\"Selenium_olx_array{datetime.now().strftime('%d%m%Y')}.csv\")\n",
    "#display(len(df))\n",
    "display(pd.DataFrame(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array[81:150]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
